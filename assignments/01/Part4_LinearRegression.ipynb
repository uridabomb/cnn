{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Linear Regression\n",
    "<a id=part4></a>\n",
    "$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial {#1}}{\\partial {#2}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll perform the classic machine learning task of linear regression.\n",
    "We'll do some simple data exploration, and implement our solution using some very widely used python libraries \n",
    "([`numpy`](https://docs.scipy.org/doc/numpy-1.15.1/reference/),\n",
    "[`scikit-learn`](http://scikit-learn.org/stable/documentation.html) and\n",
    "[`pandas`](http://pandas.pydata.org/pandas-docs/stable/))\n",
    "in order to gain some basic experience with them.\n",
    "We'll explore the generalization capacity of the model and perform cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "np.random.seed(42)\n",
    "test = unittest.TestCase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset exploration\n",
    "<a id=part4_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with the Boston housing dataset. This is a famous dataset for benchmarking regression algorithms.\n",
    "\n",
    "The dataset contains 506 samples of median house values in Boston, each with 13 associated house and neighborhood attributes (i.e. features).\n",
    "(see [here](http://scikit-learn.org/stable/datasets/index.html#boston-dataset) for their meaning).\n",
    "The 13 features of each house are our independent variables, and  we're trying to predict the value of `MEDV`, the median house price (in units of $1000).\n",
    "\n",
    "Run the following block to load the data. Since this dataset is very small, we can load it directly into memory and forgo any lazy-loading mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "# Load data we'll work with - Boston housing dataset\n",
    "# We'll use sklearn's built-in data\n",
    "ds_boston = sklearn.datasets.load_boston()\n",
    "feature_names = ds_boston.feature_names\n",
    "n_features = len(feature_names)\n",
    "x, y = ds_boston.data, ds_boston.target\n",
    "n_samples = len(y)\n",
    "print(f'Loaded {n_samples} samples')\n",
    "\n",
    "# Load into a pandas dataframe and show some samples\n",
    "df_boston = pd.DataFrame(data=x, columns=ds_boston.feature_names)\n",
    "df_boston = df_boston.assign(MEDV=y)\n",
    "df_boston.head(10).style.background_gradient(subset=['MEDV'], high=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the data a bit by plotting a scatter matrix of every variable as a function of every other and a histogram for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df_boston, figsize=(20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart shows us (among other things) how our target variable `MEDV` behaves as a function\n",
    "of the features (bottom row). By looking at it, can you guess which relationships might be good candidates for a linear model?\n",
    "\n",
    "Let's use a simple method for deciding which features to use for our linear model:\n",
    "the [correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient),\n",
    "defined as\n",
    "\n",
    "$$\n",
    "\\rho_{\\vec{x}\\vec{y}}\n",
    "= \\frac{\\sigma_{\\vec{x}\\vec{y}}}{\\sigma_{\\vec{x}} \\sigma_{\\vec{y}}}\n",
    "= \\frac\n",
    "    {\\sum_{i=1}^{N} (x_i - \\mu_\\vec{x}) (y_i - \\mu_\\vec{y}) }\n",
    "    {\\sqrt{\\sum_{i=1}^{N} (x_i - \\mu_\\vec{x})^2} \\cdot \\sqrt{\\sum_{i=1}^{N} (y_i - \\mu_\\vec{y})^2}}\n",
    "$$\n",
    "\n",
    "Where $\\vec{x}, \\vec{y}$ are $N$ samples of two variables and $\\mu, \\sigma$ refer to **sample** means and (co-)variances respectively.\n",
    "The value of $\\rho$ is $\\pm 1$ for perfect positive or negative linear relationships ($y=ax+b$),\n",
    "and somewhere in between when it's not perfect.\n",
    "Note that this coefficient is rather limited: even when $\\rho=0$, the variables may be highly dependent,\n",
    "just not in  a linear fashion.\n",
    "\n",
    "Let's implement this method to find out which features we should include in our initial linear model.\n",
    "\n",
    "**TODO** Implement the `top_correlated_features()` function in the `hw1/linear_regression.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw1.linear_regression as hw1linreg\n",
    "\n",
    "n_top_features = 5\n",
    "top_feature_names, top_corr = hw1linreg.top_correlated_features(df_boston, 'MEDV', n_top_features)\n",
    "print('Top features: ', top_feature_names)\n",
    "print('Top features correlations: ', top_corr)\n",
    "\n",
    "# Tests\n",
    "test.assertEqual(len(top_feature_names), n_top_features)\n",
    "test.assertEqual(len(top_corr), n_top_features)\n",
    "test.assertAlmostEqual(np.sum(np.abs(top_corr)), 2.893, delta=1e-3) # compare to precomputed value for n=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model\n",
    "<a id=part4_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably the simplest machine learning model is linear regression.\n",
    "We are given a dataset $\\left\\{\\vec{x}^{(i)}, y^{(i)}\\right\\}_{i=1}^{N}$ where $\\vec{x}^{(i)} \\in \\set{R}^D$\n",
    "is a $D$-dimensional feature vector and $y^{(i)}\\in\\set{R}$ is a continuous quantity assumed to be the\n",
    "output of some unknown function, i.e. $y^{(i)} = f(\\vec{x}^{(i)})$.\n",
    "\n",
    "Our goal will be to fit a linear transformation,\n",
    "parametrized by weights vector and bias term $\\vec{w}, b$, such that given a sample $\\vec{x}$ our prediction is \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\vectr{w}\\vec{x} + b.\n",
    "$$\n",
    "\n",
    "We'll judge the performance of the model using the ordinary least-squares sense,\n",
    "i.e. with a loss function of given by the mean-squared error (MSE) with the addition\n",
    "of an L2-regularization term:\n",
    "$$\n",
    "L(\\vec{w})\n",
    "= \\frac{1}{N} \\sum_{i=1}^{N} \\left( y^{(i)} - \\hat{y}^{(i)} \\right)^2 + \\lambda\\norm{\\vec{w}}^2_2\n",
    "= \\frac{1}{N} \\sum_{i=1}^{N} \\left( y^{(i)} - \\vectr{w}\\vec{x}^{(i)} - b \\right)^2 + \\lambda\\norm{\\vec{w}}^2_2.\n",
    "$$\n",
    "\n",
    "Minimizing the above $L(\\vec{w})$ is a simple convex optimization problem\n",
    "with a closed-form solution. Of course, this can also be solved using iterative descent methods which\n",
    "are necessary when the data is too large to fit in memory.\n",
    "\n",
    "As a warm up, let's implement the bias trick (this time as a `sklearn` transformation)\n",
    "so that our linear regression model will operate on data with an added bias term.\n",
    "\n",
    "**TODO** Implement the class `BiasTrickTransformer` in the `hw1/linear_regression.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BiasTrickTransformer\n",
    "bias_tf = hw1linreg.BiasTrickTransformer()\n",
    "\n",
    "xt1 = np.random.randint(10, 20, size=(5,2))\n",
    "xt2 = np.random.randn(10, 1)\n",
    "for xt in (xt2, xt1):\n",
    "    xb = bias_tf.fit_transform(xt)\n",
    "    test.assertEqual(xb.ndim, 2)\n",
    "    test.assertTrue(np.all(xb[:,0] == 1))\n",
    "    test.assertTrue(np.all(xb[:, 1:] == xt))\n",
    "    \n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now define a function to assess the accuracy of our models prediction (loss and score).\n",
    "We'll use the MSE loss as above and [$R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination) as a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(y: np.ndarray, y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Calculates mean squared error (MSE) and coefficient of determination (R-squared).\n",
    "    :param y: Target values.\n",
    "    :param y_pred: Predicted values.\n",
    "    :return: A tuple containing the MSE and R-squared values.\n",
    "    \"\"\"\n",
    "    mse = np.mean((y - y_pred) ** 2)\n",
    "    rsq = 1 - mse / np.var(y)\n",
    "    return mse.item(), rsq.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement our model.\n",
    "\n",
    "**TODO** Based on the above equations, implement the `predict()` and `fit()`\n",
    "functions in the `LinearRegressor` class within the module `linear_regression.py`.\n",
    "You'll need to first derive the closed-form solution for the optimal $\\vec{w}$ based on the loss.\n",
    "Run the code block below to fit your model to each of the 5 top\n",
    "features you selected (one at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.pipeline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=n_top_features, sharey=True, figsize=(20,5))\n",
    "actual_mse = []\n",
    "\n",
    "# Create our model as a pipline: First the bias trick is applied, then the regressor\n",
    "model = sklearn.pipeline.make_pipeline(\n",
    "    hw1linreg.BiasTrickTransformer(),\n",
    "    hw1linreg.LinearRegressor(),\n",
    ")\n",
    "\n",
    "# Fit a single feature at a time\n",
    "for i, feature_name in enumerate(top_feature_names):\n",
    "    xf = df_boston[feature_name].values.reshape(-1, 1)\n",
    "\n",
    "    y_pred = model.fit_predict(xf, y)\n",
    "    mse, rsq = evaluate_accuracy(y, y_pred)\n",
    "\n",
    "    x_line = np.arange(xf.min(), xf.max(), 0.1, dtype=np.float).reshape(-1, 1)\n",
    "    y_line = model.predict(x_line)\n",
    "\n",
    "    # Plot\n",
    "    ax[i].scatter(xf, y, marker='o', edgecolor='black')\n",
    "    ax[i].plot(x_line, y_line, color='red', lw=2, label=f'fit, $R^2={rsq:.2f}$')\n",
    "    ax[i].set_ylabel('MEDV')\n",
    "    ax[i].set_xlabel(feature_name)\n",
    "    ax[i].legend()\n",
    "    \n",
    "    actual_mse.append(mse)\n",
    "\n",
    "# Test regressor implementation\n",
    "expected_mse = [38.483, 43.6, 62.652, 64.666, 65.887]\n",
    "for i in range(len(expected_mse)):\n",
    "    test.assertAlmostEqual(expected_mse[i], actual_mse[i], delta=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the results are not great. We can't reliably predict the target variable based on just one of these.\n",
    "Now let's fit a model based on the combined top-5 features.\n",
    "Since it's difficult to visualize high-dimensional hyperplanes,\n",
    "instead of plotting the data and fitted hyperplane, we'll create a **residuals** plot. This is the plot of the error, or residual $e^{(i)} = y^{(i)} - \\hat{y}^{(i)}$ vs. the predicted value $\\hat{y}^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit top-5 features\n",
    "xf_top = df_boston[top_feature_names].values\n",
    "\n",
    "y_pred = model.fit_predict(xf_top, y)\n",
    "mse5, rsq5 = evaluate_accuracy(y, y_pred)\n",
    "print(f'mse5={mse5:.2f}, rsq5={rsq5:.2f}')\n",
    "\n",
    "# Residuals plot\n",
    "def plot_residuals(y, y_pred, ax=None, res_label=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    res = y - y_pred\n",
    "    ax.scatter(y_pred, y_pred-y, marker='s', edgecolor='black', label=res_label)\n",
    "    ax.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), color='red', lw=3)\n",
    "    ax.hlines(y=[-res.std(), res.std()], xmin=y_pred.min(), xmax=y_pred.max(), color='red', lw=3, linestyles=':')\n",
    "    ax.set_xlabel(r'$\\hat{y}$')\n",
    "    ax.set_ylabel(r'$y - \\hat{y}$')\n",
    "    if res_label is not None:\n",
    "        ax.legend()\n",
    "    return ax\n",
    "\n",
    "plot_residuals(y, y_pred)\n",
    "\n",
    "test.assertLess(mse5, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better, but there's still more to be desired. Let's try to improve our model further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding nonlinear features\n",
    "<a id=part4_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many of the relationships between our features and target variable are obviously not linear and\n",
    "cannot be modeled completely by fitting lines (or hyperplanes).\n",
    "Is there a way to fit a non-linear function to the data (such as a polynomial) but still use the simplicity of the\n",
    "Linear Regression model?\n",
    "\n",
    "A Linear Regression model actually only needs to be linear in the **parameters** of the model.\n",
    "It doesn't care what possibly nonlinear relationships exists between the feature values. This means that\n",
    "we can increase the **capacity** of our model (its ability to fit a wide variety of functions) by adding\n",
    "more parameters that correspond to non-linear transformations of the features. \n",
    "\n",
    "Suppose we have 2-dimensional feature vectors, $\\vec{x}=(x_1, x_2)$.\n",
    "We can fit a linear regression model with 3 parameters which represent some 2-d plane.\n",
    "However if we transform each such feature vector, for example by\n",
    "$\\vec{\\tilde{x}} = (x_1, x_2, x_1^2, x_1 x_2, x_2^2)$ then we can now fit a model with 6 parameters to\n",
    "the same data. These parameters still represent a hyperplane, but in a higher dimensional space which\n",
    "combines the original features. Thus, when looking at the prediction surface in the original 2-d space,\n",
    "we'll see that the model actually fitted a non-linear surface instead of a 2-d plane - in this example\n",
    "a second order polynomial. The model is still linear, since the prediction method is now \n",
    "$\\hat{y} = \\vectr{w}\\vec{\\tilde{x}} + b$, but it's capacity has been increased.\n",
    "\n",
    "Let's implement some hand-crafted nonlinear features based on all the features in the dataset.\n",
    "This step in the machine learning process is sometimes also referred to as feature engineering.\n",
    "In the rest of the course, you'll see how Deep Learning\n",
    "allows us to learn the features themselves instead of creating them by hand, and thus creating very\n",
    "powerful representations.\n",
    "\n",
    "**TODO** Implement the `BostonFeaturesTransformer` class in the `hw1/linear_regression.py` module.\n",
    "You can create any features you want, for example given $\\vec{x}=(x_1,x_2)$ you could generate features\n",
    "such as $x_1^2$, $x_1 \\log{x_2}$,  $e^{-x_1}$ and so on.\n",
    "Try to infer relationships based on the scatter matrix.\n",
    "\n",
    "Notes:\n",
    "- You can use the class `PolynomialFeatures` from `sklearn.preprocessing`\n",
    "  to simplify generation of polynomial features.\n",
    "- Removing a feature is also a new feature. You can discard features if they're not helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_boston(model, x, y, fit=True):\n",
    "    if fit:\n",
    "        model.fit(x, y)\n",
    "    y_pred = model.predict(x)\n",
    "    mse, rsq = evaluate_accuracy(y, y_pred)\n",
    "    return y_pred, mse, rsq\n",
    "\n",
    "# Fit with all features this time\n",
    "x = df_boston[feature_names].values\n",
    "\n",
    "# Use model with a custom features transform\n",
    "model = sklearn.pipeline.make_pipeline(\n",
    "    hw1linreg.BiasTrickTransformer(),\n",
    "    hw1linreg.BostonFeaturesTransformer(),\n",
    "    hw1linreg.LinearRegressor()\n",
    ")\n",
    "\n",
    "y_pred, mse, rsq = linreg_boston(model, x, y)\n",
    "print(f'mse={mse:.2f}, rsq={rsq:.2f}')\n",
    "\n",
    "# Test: You should get at least 2x lower loss than previously, easily even lower\n",
    "plot_residuals(y, y_pred)\n",
    "test.assertLess(mse, mse5 / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization\n",
    "<a id=part4_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, your model should produce fairly accurate predictions.\n",
    "Note howerver that we trained it on the entire Boston dataset.\n",
    "\n",
    "When training models, we don't actually care about their performance on the training data;\n",
    "we're not interested in solving optimization problems.\n",
    "What we want is the ability to **generalize**: How well will it perform on novel, unseen data?\n",
    "In other words, did the model learn some function similar to the one actually generating the samples?\n",
    "\n",
    "Let's find out how good our model is for unseen data the usual way: We'll split our dataset into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data and model\n",
    "x = df_boston[feature_names].values\n",
    "y = df_boston['MEDV'].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "model = sklearn.pipeline.make_pipeline(\n",
    "    hw1linreg.BiasTrickTransformer(),\n",
    "    hw1linreg.BostonFeaturesTransformer(),\n",
    "    hw1linreg.LinearRegressor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, instead of just fitting the model on the training set and evaluating on the test set,\n",
    "we'll use cross-validation to find a set of model hyperparameters that allow the model to generalize well.\n",
    "We'll again use k-fold CV to split the training set into k-folds where for each set of hyperparameters being tested,\n",
    "each time one of the folds is treated like the test set and the model is fitted to the rest.\n",
    "\n",
    "**TODO** Implement the `cv_best_hyperparams()` function in the `hw1/linear_regression.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search-spaces for hyper parameters\n",
    "degree_range = np.arange(1, 4)\n",
    "lambda_range = np.logspace(-3, 2, base=10, num=20)\n",
    "\n",
    "# Use cross-validation to find best combination of hyperparameters\n",
    "best_hypers = hw1linreg.cv_best_hyperparams(model, x_train, y_train, k_folds=3,\n",
    "                                            degree_range=degree_range, lambda_range=lambda_range)\n",
    "\n",
    "print('Best hyperparameters: ', best_hypers)\n",
    "\n",
    "# Make sure returned params exist in the model\n",
    "for param in best_hypers.keys():\n",
    "    test.assertIn(param, model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use the best hyperparameters to train a model on the training set and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best hyperparameters\n",
    "model.set_params(**best_hypers)\n",
    "\n",
    "# Train best model on full training set\n",
    "y_pred_train, mse, rsq = linreg_boston(model, x_train, y_train)\n",
    "print(f'train: mse={mse:.2f}, rsq={rsq:.2f}')\n",
    "ax = plot_residuals(y_train, y_pred_train, res_label='train')\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test, mse, rsq = linreg_boston(model, x_test, y_test, fit=False)\n",
    "print(f'test:  mse={mse:.2f}, rsq={rsq:.2f}')\n",
    "ax = plot_residuals(y_test, y_pred_test, ax=ax, res_label='test')\n",
    "\n",
    "# Make sure test-set accuracy is sane\n",
    "test.assertLess(mse, 25) # You should be able to get way below this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw1/answers.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.answers import display_answer\n",
    "import hw1.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 \n",
    "\n",
    "Whats the ideal pattern to see in a residual plot?\n",
    "Based on the residual plots you got above, what can you say about the fitness of the trained model?\n",
    "Compare the plot for the top-5 features with the final plot after CV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_answer(hw1.answers.part4_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the cross-validation:\n",
    "\n",
    "1. When defining the range for $\\lambda$ the in the above CV code, why do you think we used\n",
    "   `np.logspace` instead of `np.linspace`?\n",
    "1. How many times in total was the model fitted to data (with the parameters as given and not including the final fit on the entire training set)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_answer(hw1.answers.part4_q2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
